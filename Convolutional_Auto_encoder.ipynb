{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Convolutional Auto encoder",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HeshanDissanayake/Convolutional_Auto_encoder/blob/main/Convolutional_Auto_encoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYVl1TLIGxMj"
      },
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "lr = 0.001\n",
        "dashes = '---------------------------'\n",
        "batchSize = 50\n",
        "def autoEncoder(input_dim, kernal_dims, n_kernals):\n",
        "\n",
        "    \"\"\" :param input_dim: dimensions of the input\n",
        "        :param kernal_dim: list of filter dimensoins\n",
        "        :param n_kernal:   list of the number of filters in each layer\n",
        "        :return: output tensor\n",
        "    \"\"\"\n",
        "\n",
        "    X = tf.placeholder(tf.float32, input_dim, name= 'inputs')\n",
        "    input = X\n",
        "\n",
        "    layer_dims = [] # a list to store the dimensions of layers to use in decoder construction\n",
        "    print('Constructing the auto encoder')\n",
        "    for layer_idx, kernal in enumerate(kernal_dims):\n",
        "\n",
        "        layer_dims.append(input.shape)\n",
        "        in_chanels = input.get_shape().as_list()[3]\n",
        "        out_chanels = n_kernals[layer_idx]\n",
        "\n",
        "        print(dashes,'\\n','convolution == %dx%dx%d'%(input.shape[1],input.shape[2],input.shape[3]))\n",
        "\n",
        "        w = tf.Variable(tf.random_uniform([kernal, kernal, in_chanels, out_chanels], -1/in_chanels, 1/in_chanels))\n",
        "        b = tf.Variable(tf.zeros([out_chanels]))\n",
        "\n",
        "        input = tf.add(tf.nn.conv2d(input, w, strides=[1, 2, 2, 1], padding='SAME'),b)\n",
        "        input = tf.nn.relu(input)\n",
        "    latent = tf.identity(input, name = 'latent')\n",
        "    # temp = input.get_shape().as_list()\n",
        "    # latent = tf.reshape(input,[-1,1,temp[1]*temp[2]*temp[3]], name='latent')\n",
        "\n",
        "    # print(dashes, '\\n', 'convolution == %dx%dx%d' % (input.shape[1], input.shape[2], input.shape[3]))\n",
        "    # print(dashes, '\\n', 'latent space == %dx%d'%(latent.shape[1],latent.shape[2]))\n",
        "\n",
        "\n",
        "    # input = tf.reshape(latent,[tf.shape(X)[0],temp[1],temp[2],temp[3]])\n",
        "\n",
        "    layer_dims.reverse()\n",
        "    kernal_dims.reverse()\n",
        "    n_kernals.reverse()\n",
        "\n",
        "    for layer_idx, kernal in enumerate(kernal_dims):\n",
        "\n",
        "        in_chanels = input.get_shape().as_list()[3]\n",
        "        out_chanels = layer_dims[layer_idx].as_list()[3]\n",
        "        print(dashes,'\\n','convolution == %dx%dx%d'%(input.shape[1],input.shape[2],input.shape[3]))\n",
        "\n",
        "\n",
        "        w = tf.Variable(tf.random_uniform([kernal, kernal, out_chanels, in_chanels],\n",
        "                                          -1/in_chanels, 1/in_chanels, tf.float32))\n",
        "\n",
        "        b = tf.Variable(tf.zeros([out_chanels]))\n",
        "        shape = layer_dims[layer_idx]\n",
        "        input = tf.nn.conv2d_transpose(input, w, [tf.shape(X)[0], shape[1], shape[2], shape[3]], strides=[1, 2, 2, 1], padding='SAME')\n",
        "        input = tf.add(input,b)\n",
        "        input = tf.nn.relu(input)\n",
        "\n",
        "    input = tf.identity(input, name = 'output')\n",
        "\n",
        "    cost =tf.reduce_sum(tf.square(X - input))\n",
        "    return {\"input\": X, \"output\":input, \"cost\":cost}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "data = np.load('data.npy')/255\n",
        "im = (data[30,:,:,:]*255).astype(np.int)\n",
        "\n",
        "n = np.zeros((1,im.shape[0],im.shape[1], 3))\n",
        "n[0,:,:,:] = data[30,:,:,:]\n",
        "\n",
        "\n",
        "plt.imshow(im)\n",
        "plt.pause(0.001)\n",
        "\n",
        "Aencoder = autoEncoder([None,n.shape[1],n.shape[2],3],list([10, 5, 5, 5]),list([10, 20, 30, 1]))\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate = 0.0001).minimize(Aencoder[\"cost\"])\n",
        "ses= tf.Session()\n",
        "ses.run(tf.global_variables_initializer())\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "for i in range(1000000):\n",
        "    for j in range(0,9000, 50):\n",
        "        miniBatch = data[j:j+50,:,:,:]\n",
        "        ses.run(optimizer, feed_dict={Aencoder['input']: miniBatch})\n",
        "        \n",
        "    print('ecpoc',i,'cost is:',ses.run(Aencoder['cost'],feed_dict={Aencoder['input']:data[9000:9200,:,:,:]}))\n",
        "    if i%100 == 0:\n",
        "        xx = ses.run(Aencoder[\"output\"], feed_dict={Aencoder[\"input\"]: n})\n",
        "        print('Saving the model')\n",
        "        saver.save(ses, './AE')\n",
        "        plt.imshow((xx[0, :, :, :]*255).astype(np.int))\n",
        "        print((xx[0, :, :, :]*255).astype(np.int))\n",
        "        plt.pause(0.001)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9V1kucHJDnv"
      },
      "source": [
        "im = cv2.imread('/content/me.jpg')\n",
        "print(im)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Erb725r5HSkW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7f0717bd-e2e9-4534-e757-f65e2cc1bd85"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veUS65geHpyM"
      },
      "source": [
        "!cp -R checkpoint /content/gdrive/My\\ Drive/checkpoint\n",
        "!cp -R AE.data-00000-of-00001\t /content/gdrive/My\\ Drive/AE.data-00000-of-00001\t\n",
        "!cp -R AE.meta  /content/gdrive/My\\ Drive/AE.meta \n",
        "!cp -R AE.index  /content/gdrive/My\\ Drive/AE.index \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvINaU6Ozqwm"
      },
      "source": [
        "!cp -R  /content/gdrive/My\\ Drive/data.npy data.npy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKxDBEdXJvTi"
      },
      "source": [
        "!rm AE.index AE.meta AE.data-00000-of-00001 checkpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3_5Ysm_ncmD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e951745e-48db-4d42-e776-bf4280a1a245"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcHLr1pgpsOr"
      },
      "source": [
        "n[0,:,:,:] = data[30,:,:,:]/255\n",
        "n = n.astype(np.float32)\n",
        "print(n.dtype)\n",
        "sess = tf.Session()\n",
        "\n",
        "meta = tf.train.import_meta_graph('./AE.meta')\n",
        "meta.restore(sess, ('./AE'))\n",
        "print(\"buf1\")\n",
        "\n",
        "graph = tf.get_default_graph()\n",
        "input = graph.get_tensor_by_name(\"inputs:0\")\n",
        "output = graph.get_tensor_by_name(\"output:0\")\n",
        "print(input)\n",
        "\n",
        "x = sess.run(output, feed_dict={input : n})\n",
        "x = (x*1000).astype(np.int)\n",
        "print(x[0, :, :, :])\n",
        "plt.imshow(x[0, :, :, :])\n",
        "plt.pause(10)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}